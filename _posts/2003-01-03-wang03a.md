---
title: Latent Maximum Entropy Approach for Semantic $N$-gram Language Modeling
abstract: In this paper, we describe a unified probabilistic framework for statistical
  language modeling-the latent maximum entropy principle-which can effectively incorporate
  various aspects of natural language, such as local word interaction, syntactic structure
  and semantic document information. Unlike previous work on maximum entropy methods
  for language modeling, which only allow explicit features to be modeled, our framework
  also allows relationships over hidden features to be captured, resulting in a more
  expressive language model. We describe efficient algorithms for marginalization,
  inference and normalization in our extended models. We then present promising experimental
  results for our approach on the Wall Street Journal corpus.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang03a
month: 0
tex_title: Latent Maximum Entropy Approach for Semantic $N$-gram Language Modeling
firstpage: 316
lastpage: 322
page: 316-322
order: 316
cycles: false
bibtex_editor: Bishop, Christopher M. and Frey, Brendan J.
editor:
- given: Christopher M.
  family: Bishop
- given: Brendan J.
  family: Frey
bibtex_author: Wang, Shaojun and Schuurmans, Dale and Peng, Fuchun
author:
- given: Shaojun
  family: Wang
- given: Dale
  family: Schuurmans
- given: Fuchun
  family: Peng
date: 2003-01-03
note: Reissued by PMLR on 01 April 2021.
address:
container-title: Proceedings of the Ninth International Workshop on Artificial Intelligence
  and Statistics
volume: R4
genre: inproceedings
issued:
  date-parts:
  - 2003
  - 1
  - 3
pdf: http://proceedings.mlr.press/r4/wang03a/wang03a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
