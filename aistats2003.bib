@proceedings{aistats2003,
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  booktitle     = {Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics},
  shortname = {AISTATS},
  name = {International Workshop on Artificial Intelligence and Statistics},
  publisher = {Society for Artificial Intelligence and Statistics},
  year      = {2003},
  url       = {http://research.microsoft.com/conferences/aistats2003/},
  isbn      = {0-9727358-0-1},
  timestamp = {Wed, 06 May 2015 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/conf/aistats/2003.bib},
  address = {Key West, Florida, USA},
  start = {2003-01-03},
  end = {2003-01-06},
  published = {2021-04-01},
  firstpublished = {2003-01-03},
  url = {http://aistats.org/aistats2003/}
}


@InProceedings{183,
  author    = {Kim E. Andersen and Malene H{\o}jbjerre},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {A Bayesian Approach to Bergman's Minimal Model},
  pages     = {1-8},
  abstract  = {The classical minimal model of glucose disposal was proposed as a powerful modeling approach to estimating the insulin sensitivity and the glucose effectiveness, which are very useful in the study of diabetes. The minimal model is a highly ill-posed inverse problem and most often the reconstruction of the glucose kinetics has been done by deterministic iterative numerical algorithms. However, these algorithms do not consider the severe ill-posedness inherent in the minimal model and may only be efficient when a good initial estimate is provided. In this work we adopt graphical models as a powerful and flexible modeling framework for regularizing the problem and thereby allow for estimation of the insulin sensitivity and glucose effectiveness. We illustrate how the reconstruction algorithm may be efficiently implemented in a Bayesian approach where posterior sampling is made through the use of Markov chain Monte Carlo techniques. We demonstrate the method on simulated data.}
}

@InProceedings{206,
  author    = {Hagai Attias},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Planning by Probabilistic Inference},
  pages     = {9-16},
  abstract  = {This paper presents and demonstrates a new approach to the problem of planning under uncertainty. Actions are treated as hidden variables, with their own prior distributions, in a probabilistic generative model involving actions and states. Planning is done by computing the posterior distribution over actions, conditioned on reaching the goal state within a specified number of steps. Under the new formulation, the toolbox of inference techniques be brought to bear on the planning problem. This paper focuses on problems with discrete actions and states, and discusses some extensions.}
}

@InProceedings{164,
  author    = {Yoshua Bengio and Jean-S{\'{e}}bastien Senecal},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Quick Training of Probabilistic Neural Nets by Importance Sampling},
  pages     = {17-24},
  abstract  = {Our previous work on statistical language modeling introduced the use of probabilistic feedforward neural networks to help dealing with the curse of dimensionality. Training this model by maximum likelihood however requires for each example to perform as many network passes as there are words in the vocabulary. Inspired by the contrastive divergence model, we propose and evaluate sampling-based methods which require network passes only for the observed "positive example" and a few sampled negative example words. A very significant speed-up is obtained with an adaptive importance sampling.}
}

@InProceedings{152,
  author    = {Christopher M. Bishop and Andrew Blake and Bhaskara Marthi},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Super-resolution Enhancement of Video},
  pages     = {25-32},
  abstract  = {We consider the problem of enhancing the resolution of video through the addition of perceptually plausible high frequency information. Our approach is based on a learned data set of image patches capturing the relationship between the middle and high spatial frequency bands of natural images. By introducing an appropriate prior distribution over such patches we can ensure consistency of static image regions across successive frames of the video, and also take account of object motion. A key concept is the use of the previously enhanced frame to provide part of the training set for super-resolution enhancement of the current frame. Our results show that a marked improvement in video quality can be achieved at reasonable computational cost.}
}

@InProceedings{187,
  author    = {Christopher M. Bishop and John M. Winn},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Structured Variational Distributions in {VIBES}},
  pages     = {33-40},
  abstract  = {Variational methods are becoming increasingly popular for the approximate solution of complex probabilistic models in machine learning, computer vision, information retrieval and many other fields. Unfortunately, for every new application it is necessary first to derive the specific forms of the variational update equations for the particular probabilistic model being used, and then to implement these equations in applicationspecific software. Each of these steps is both time consuming and error prone. We have therefore recently developed a general purpose inference engine called VIBES [1] ('Variational Inference for Bayesian Networks') which allows a wide variety of probabilistic models to be implemented and solved variationally without recourse to coding. New models are specified as a directed acyclic graph using an interface analogous to a drawing package, and VIBES then automatically generates and solves the variational equations. The original version of VIBES assumed a fully factorized variational posterior distribution. In this paper we present an extension of VIBES in which the variational posterior distribution corresponds to a sub-graph of the full probabilistic model. Such structured distributions can produce much closer approximations to the true posterior distribution. We illustrate this approach using an example based on Bayesian hidden Markov models.}
}

@InProceedings{189,
  author    = {Matthew Brand and Kun Huang},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {A Unifying Theorem for Spectral Embedding and Clustering},
  pages     = {41-48},
  abstract  = {Spectral methods use selected eigenvectors of a data affinity matrix to obtain a data representation that can be trivially clustered or embedded in a low-dimensional space. We present a theorem that explains, for broad classes of affinity matrices and eigenbases, why this works:
For successively smaller eigenbases (i.e., using fewer and fewer of the affinity matrix's dominant eigenvalues and eigenvectors), the angles between "similar" vectors in the new representation shrink while the angles between "dissimilar" vectors grow. Specifically, the sum of the squared cosines of the angles is strictly increasing as the dimensionality of the representation decreases. Thus spectral methods work because the truncated eigenbasis amplifies structure in the data so that any heuristic post-processing is more likely to succeed. We use this result to construct a nonlinear dimensionality reduction (NLDR) algorithm for data sampled from manifolds whose intrinsic coordinate system has linear and cyclic axes, and a novel clustering-by-projections algorithm that requires no post-processing and gives superior performance on "challenge problems" from the recent literature.}
}

@InProceedings{163,
  author    = {Eric Brochu and Nando de Freitas and Kejie Bao},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {The Sound of an Album Cover: A Probabilistic Approach to Multimedia},
  pages     = {49-56},
  abstract  = {We present a novel, flexible, statistical approach to modeling music, images and text jointly. The technique is based on multi-modal mixture models and efficient computation using online EM. The learned models can be used to browse multimedia databases, to query on a multimedia database using any combination of music, images and text (lyrics and other contextual information), to annotate documents with music and images, and to find documents in a database similar to input text, music and/or graphics files.}
}

@InProceedings{207,
  author    = {Wray L. Buntine and Sami Perttu},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Is Multinomial {PCA} Multi-faceted Clustering or Dimensionality Reduction?},
  pages     = {57-64},
  abstract  = {Discrete analogues to Principal Components Analysis (PCA) are intended to handle discrete or positive-only data, for instance sets of documents. The class of methods is appropriately called multinomial PCA because it replaces the Gaussian in the probabilistic formulation of PCA with a multinomial. Experiments to date, however, have been on small data sets, for instance, from early information retrieval collections. This paper demonstrates the method on two large data sets and considers two extremes of behaviour: (1) dimensionality reduction where the feature set (i.e., bag of words) is considerably reduced, and (2) multi-faceted clustering (or aspect modelling) where clustering is done but items can now belong in several clusters at once.}
}

@InProceedings{201,
  author    = {Shantanu Chakrabartty and Gert Cauwenberghs},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Expectation Maximization of Forward Decoding Kernel Machines},
  pages     = {65-71},
  abstract  = {Forward Decoding Kernel Machines (FDKM) combine large-margin kernel classifiers with Hidden Markov Models (HMM) for Maximum a Posteriori (MAP) adaptive sequence estimation. This paper proposes a variant on FDKM training using ExpectationMaximization (EM). Parameterization of the expectation step controls the temporal extent of the context used in correcting noisy and missing labels in the training sequence. Experiments with EM-FDKM on TIMIT phone sequence data demonstrate up to $10 \%$ improvement in classification performance over FDKM trained with hard transitions between labels.}
}

@InProceedings{173,
  author    = {Denver Dash and Gregory F. Cooper},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Model Averaging with Bayesian Network Classifiers},
  pages     = {72-79},
  abstract  = {This paper considers the problem of performing classification by model-averaging over a class of discrete Bayesian network structures consistent with a partial ordering and with bounded in-degree $k .$ We show that for $N$ nodes this class contains in the worst-case at least $\Omega\left(\left(\begin{array}{c}N/2 \\ k\end{array}\right)^{N / 2} \right)$ distinct network structures, \right. but we show that this summation can be performed in $O\left(\left(\begin{array}{c}N \\ k\end{array}\right) \cdot N\right)$  time. We use this fact to show that it is possible to efficiently construct a single directed acyclic graph (DAG) whose predictions approximate those of exact model-averaging over this class, allowing approximate model-averaged predictions to be performed in $O(N)$ time. We evaluate the procedure in a supervised classification context, and show empirically that this technique can be beneficial for classification even when the generating distribution is not a member of the class being averaged over, and we characterize the performance over several parameters on simulated and real-world data.}
}

@InProceedings{188,
  author    = {A. Philip Dawid},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {An object-oriented Bayesian network for estimating mutation rates},
  pages     = {80-84},
  abstract  = {We describe the use of the object-oriented HUGIN 6 probabilistic expert system software to structure the problem of estimating mutation rates on the basis of family data when paternity can not be regarded as certain.}
}

@InProceedings{140,
  author    = {Chris Ding},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Document Retrieval and Clustering: from Principal Component Analysis to Self-aggregation Networks},
  pages     = {85-92},
  abstract  = {Abstract. We first extend Hopfield networks to clustering bipartite graphs (words-to-document association) and show that the solution is the principal component analysis. We then generalize this via the min-max clustering principle into a self-aggregation networks which are composed of scaled PCA components via Hebb rule. Clustering amounts to an updating process where connections between different clusters are automatically suppressed while connections within same clusters are enhanced. This framework combines dimension reduction with clustering via neural networks and PCA. Self-aggregation networks can also improve information retrieval performance. Applications are presented.}
}

@InProceedings{165,
  author    = {Susana Eyheramendy and David D. Lewis and David Madigan},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {On the Naive Bayes Model for Text Categorization},
  pages     = {93-100},
  abstract  = {This paper empirically compares the performance of four probabilistic models for text classification - Poisson, Bernoulli, Multinomial and Negative Binomial. We examine the "naive Bayes" assumption in the four models and show that the multinomial model is a modified naive Bayes Poisson model that assumes independence of document length and document class. Despite the fact that this last assumption might not be correct in many situations, we find that, in general, relaxing it does not change the performance of the classifier. Finally we propose and evaluate an ad-hoc method for incorporating document length.}
}

@InProceedings{181,
  author    = {Scott Gaffney and Padhraic Smyth},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Curve Clustering with Random Effects Regression Mixtures},
  pages     = {101-108},
  abstract  = {In this paper we address the problem of clustering sets of curve or trajectory data generated by groups of objects or individuals. The focus is to model curve data directly using a set of model-based curve clustering algorithms referred to as mixtures of regressions or regression mixtures. The proposed methodology is based on extension to regression mixtures that we call random effects regression mixtures which combines linear random effects models with standard regression mixtures. We develop a general expectationmaximization (EM) algorithm using maximum a posteriori (MAP) estimation for random effects regression mixtures and demonstrate how this technique can be applied to the problem of clustering cyclone data.}
}

@InProceedings{158,
  author    = {Xianping Ge and Sridevi Parise and Padhraic Smyth},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Clustering Markov States into Equivalence Classes using {SVD} and Heuristic Search Algorithms},
  pages     = {109-116},
  abstract  = {}
}

@InProceedings{180,
  author    = {Alexander G. Gray and Andrew W. Moore},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Rapid Evaluation of Multiple Density Models},
  pages     = {117-123},
  abstract  = {}
}

@InProceedings{166,
  author    = {Paul Gustafson and Peter Carbonetto and Natalie Thompson and Nando de Freitas},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Bayesian Feature Weighting for Unsupervised Learning, with Application to Object Recognition},
  pages     = {124-131},
  abstract  = {}
}

@InProceedings{121,
  author    = {Tom Heskes and Onno Zoeter},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Generalized belief propagation for approximate inference in hybrid Bayesian networks},
  pages     = {132-140},
  abstract  = {}
}

@InProceedings{126,
  author    = {Geoff Hulten and David Maxwell Chickering and David Heckerman},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Learning Bayesian Networks From Dependency Networks: {A} Preliminary Study},
  pages     = {141-148},
  abstract  = {}
}

@InProceedings{212,
  author    = {Tony Jebara},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Convex Invariance Learning},
  pages     = {149-156},
  abstract  = {}
}

@InProceedings{105,
  author    = {Jaz S. Kandola and John Shawe-Taylor},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Refining Kernels for Regression and Uneven Classification Problems},
  pages     = {157-162},
  abstract  = {}
}

@InProceedings{174,
  author    = {Paul Komarek and Andrew W. Moore},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Fast Robust Logistic Regression for Large Sparse Datasets with Binary Outputs},
  pages     = {163-170},
  abstract  = {}
}

@InProceedings{172,
  author    = {Petri Kontkanen and Wray L. Buntine and Petri Myllym{\"{a}}ki and Jorma Rissanen and Henry Tirri},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Efficient Computing of Stochastic Complexity},
  pages     = {171-178},
  abstract  = {}
}

@InProceedings{134,
  author    = {Manabu Kuroki and Zhihong Cai},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {The Joint Causal Effect in Linear Structural Equation Model and Its Application to Process Analysis},
  pages     = {179-186},
  abstract  = {}
}

@InProceedings{211,
  author    = {David Larkin and Rina Dechter},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Bayesian Inference in the Presence of Determinism},
  pages     = {187-194},
  abstract  = {}
}

@InProceedings{148,
  author    = {Juan Lin},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Reduced Rank Approximations of Transition Matrices},
  pages     = {195-202},
  abstract  = {}
}

@InProceedings{190,
  author    = {David Madigan and Yehuda Vardi and Ishay Weissman},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {On Retrieval Properties of Samples of Large Collections},
  pages     = {203-208},
  abstract  = {}
}

@InProceedings{120,
  author    = {Marina Meila},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Data Centering in Feature Space},
  pages     = {209-216},
  abstract  = {}
}

@InProceedings{167,
  author    = {Pinar Muyan and Nando de Freitas},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {A Blessing of Dimensionality: Measure Concentration and Probabilistic Inference},
  pages     = {217-224},
  abstract  = {}
}

@InProceedings{208,
  author    = {Nemanja Petrovic and Nebojsa Jojic and Brendan J. Frey and Thomas S. Huang},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Real-time On-line Learning of Transformed Hidden Markov Models from Video},
  pages     = {225-232},
  abstract  = {}
}

@InProceedings{154,
  author    = {Iead Rezek and Stephen J. Roberts and Peter Sykacek},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Ensemble Coupled Hidden Markov Models for Joint Characterisation of Dynamic Signals},
  pages     = {233-239},
  abstract  = {}
}

@InProceedings{119,
  author    = {Andrew I. Schein and Lawrence K. Saul and Lyle H. Ungar},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {A Generalized Linear Model for Principal Component Analysis of Binary Data},
  pages     = {240-247},
  abstract  = {}
}

@InProceedings{118,
  author    = {Nicol N. Schraudolph and Thore Graepel},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Combining Conjugate Direction Methods with Stochastic Approximation of Gradients},
  pages     = {248-253},
  abstract  = {}
}

@InProceedings{176,
  author    = {Matthias W. Seeger and Christopher K. I. Williams and Neil D. Lawrence},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Fast Forward Selection to Speed Up Sparse Gaussian Process Regression},
  pages     = {254-261},
  abstract  = {}
}

@InProceedings{178,
  author    = {Yee Whye Teh and Max Welling},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {On Improving the Efficiency of the Iterative Proportional Fitting Procedure},
  pages     = {262-269},
  abstract  = {}
}

@InProceedings{204,
  author    = {Bo Thiesson and Christopher Meek},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Discriminative Model Selection for Density Models},
  pages     = {270-275},
  abstract  = {}
}

@InProceedings{123,
  author    = {Michael E. Tipping and Anita C. Faul},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Fast Marginal Likelihood Maximisation for Sparse Bayesian Models},
  pages     = {276-283},
  abstract  = {The 'sparse Bayesian' modelling approach, as exemplified by the 'relevance vector machine', enables sparse classification and regression functions to be obtained by linearlyweighting a small number of fixed basis functions from a large dictionary of potential candidates. Such a model conveys a number of advantages over the related and very popular 'support vector machine', but the necessary 'training' procedure - optimisation of the marginal likelihood function is typically much slower. We describe a new and highly accelerated algorithm which exploits recently-elucidated properties of the marginal likelihood function to enable maximisation via a principled and efficient sequential addition and deletion of candidate basis functions.}
}

@InProceedings{198,
  author    = {P{\'{e}}ter Torma and Csaba Szepesv{\'{a}}ri},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Sequential Importance Sampling for Visual Tracking Reconsidered},
  pages     = {284-291},
  abstract  = {}
}

@InProceedings{147,
  author    = {Philip H. S. Torr},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Solving Markov Random Fields using Semi Definite Programming},
  pages     = {292-299},
  abstract  = {}
}

@InProceedings{133,
  author    = {Ioannis Tsamardinos and Constantin F. Aliferis},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Towards Principled Feature Selection: Relevancy, Filters and Wrappers},
  pages     = {300-307},
  abstract  = {}
}

@InProceedings{122,
  author    = {Martin J. Wainwright and Tommi S. Jaakkola and Alan S. Willsky},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Tree-reweighted Belief Propagation Algorithms and Approximate {ML} Estimation by Pseudo-Moment Matching},
  pages     = {308-315},
  abstract  = {}
}

@InProceedings{169,
  author    = {Shaojun Wang and Dale Schuurmans and Fuchun Peng},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {Latent Maximum Entropy Approach for Semantic N-gram Language Modeling},
  pages     = {316-322},
  abstract  = {}
}

@InProceedings{156,
  author    = {Abraham J. Wyner},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {On Boosting and the Exponential Loss},
  pages     = {323-329},
  abstract  = {}
}

@InProceedings{171,
  author    = {Richard S. Zemel and Craig Boutilier},
  editor    = {Christopher M. Bishop and Brendan J. Frey},
  title     = {An Active Approach to Collaborative Filtering},
  pages     = {330-337},
  abstract  = {}
}

